{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPloBG8hsGgbi35qCYu+kow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xtinc-T/html-portfolio/blob/main/Optimisation_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funtions :\n",
        "<br>\n",
        "function: 6*(x-4)^4+4*(y-4)^2\n",
        "<br>\n",
        "function: Max(x-4,0)+4*|y-4|\n",
        "\n"
      ],
      "metadata": {
        "id": "j6WWjh7x78e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import Symbol, Max, Derivative\n",
        "\n",
        "# Define symbolic variables\n",
        "x = Symbol('x')\n",
        "y = Symbol('y')\n",
        "\n",
        "# First function\n",
        "function1 = Max(x-4, 0) + 4*abs(y-4)\n",
        "\n",
        "# Differentiate function1 with respect to x (wrt x means w.r.t. x)\n",
        "d_function1_dx = Derivative(function1, x).doit()\n",
        "\n",
        "# Second function\n",
        "function2 = 6*(x-4)**4 + 4*(y-4)**2\n",
        "\n",
        "# Differentiate function2 with respect to x\n",
        "d_function2_dx = Derivative(function2, x).doit()\n",
        "\n",
        "# Print the results for x derivatives\n",
        "print(\"Derivative of the first function with respect to x:\", d_function1_dx)\n",
        "print(\"Derivative of the second function with respect to x:\", d_function2_dx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df_h6pLO8K-2",
        "outputId": "15cd0bfe-c909-4be4-da2d-b3ebc879e09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Derivative of the first function with respect to x: Heaviside(x - 4)\n",
            "Derivative of the second function with respect to x: 24*(x - 4)**3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Your first task is to implement the following gradient descent updates that we studied\n",
        "in the lectures: <br>\n",
        "(i) Polyak step size <br>\n",
        "(ii) RMSProp<br>\n",
        "(iii) Heavy Ball <br>\n",
        "(iv) Adam <br>\n"
      ],
      "metadata": {
        "id": "a7b81Ez92YVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polyak_update(params, gradients, learning_rate, beta):\n",
        "  \"\"\"\n",
        "  Updates model parameters using Polyak step size.\n",
        "\n",
        "  Args:\n",
        "      params: List of model parameters.\n",
        "      gradients: List of gradients for each parameter.\n",
        "      learning_rate: Base learning rate.\n",
        "      beta: Polyak averaging coefficient (0 < beta < 1).\n",
        "\n",
        "  Returns:\n",
        "      List of updated model parameters.\n",
        "  \"\"\"\n",
        "  updated_params = []\n",
        "  velocity = [0.0] * len(params)  # Initialize velocity with zeros\n",
        "  for i, param in enumerate(params):\n",
        "    velocity[i] = beta * velocity[i] + (1 - beta) * gradients[i]\n",
        "    updated_params.append(param - learning_rate * velocity[i])\n",
        "  return updated_params"
      ],
      "metadata": {
        "id": "VcYwg7EM37LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop_update(params, gradients, learning_rate, decay_rate, epsilon):\n",
        "  \"\"\"\n",
        "  Updates model parameters using RMSProp.\n",
        "\n",
        "  Args:\n",
        "      params: List of model parameters.\n",
        "      gradients: NumPy array of gradients for each parameter.\n",
        "      learning_rate: Base learning rate.\n",
        "      decay_rate: Decay rate for the moving average of squared gradients (0 < decay_rate < 1).\n",
        "      epsilon: Small value for numerical stability (to avoid division by zero).\n",
        "\n",
        "  Returns:\n",
        "      List of updated model parameters.\n",
        "  \"\"\"\n",
        "  updated_params = []\n",
        "  cache = np.zeros_like(params)  # Initialize cache with zeros\n",
        "  for i in range(len(params)):\n",
        "    cache[i] = decay_rate * cache[i] + (1 - decay_rate) * gradients[i] ** 2\n",
        "    updated_params.append(params[i] - learning_rate / (epsilon + np.sqrt(cache[i])) * gradients[i])\n",
        "  return updated_params\n"
      ],
      "metadata": {
        "id": "da5Ud6qi4FQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def heavy_ball_update(params, gradients, learning_rate, momentum):\n",
        "  \"\"\"\n",
        "  Updates model parameters using Heavy Ball.\n",
        "\n",
        "  Args:\n",
        "      params: List of model parameters.\n",
        "      gradients: List of gradients for each parameter.\n",
        "      learning_rate: Base learning rate.\n",
        "      momentum: Momentum coefficient (0 <= momentum < 1).\n",
        "\n",
        "  Returns:\n",
        "      List of updated model parameters.\n",
        "  \"\"\"\n",
        "  updated_params = []\n",
        "  velocity = [0.0] * len(params)  # Initialize velocity with zeros\n",
        "  for i, param in enumerate(params):\n",
        "    velocity[i] = momentum * velocity[i] + learning_rate * gradients[i]\n",
        "    updated_params.append(param - velocity[i])\n",
        "  return updated_params"
      ],
      "metadata": {
        "id": "LmQ9XXRa4H6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def adam_update(params, gradients, learning_rate, beta1, beta2, epsilon):\n",
        "  \"\"\"\n",
        "  Updates model parameters using Adam.\n",
        "\n",
        "  Args:\n",
        "      params: List of model parameters.\n",
        "      gradients: List of gradients for each parameter.\n",
        "      learning_rate: Base learning rate.\n",
        "      beta1: Decay rate for the 1st moment estimate (0 < beta1 < 1).\n",
        "      beta2: Decay rate for the 2nd moment estimate (0 < beta2 < 1).\n",
        "      epsilon: Small value for numerical stability (to avoid division by zero).\n",
        "\n",
        "  Returns:\n",
        "      List of updated model parameters.\n",
        "  \"\"\"\n",
        "  updated_params = []\n",
        "  m = [0.0] * len(params)  # Initialize 1st moment estimate with zeros\n",
        "  v = [0.0] * len(params)  # Initialize 2nd moment estimate with zeros\n",
        "  t = 0  # Initialize time step\n",
        "\n",
        "  for i, param in enumerate(params):\n",
        "    t += 1\n",
        "    m[i] = beta1 * m[i] + (1 - beta1) * gradients[i]\n",
        "    v[i] = beta2 * v[i] + (1 - beta2) * gradients[i] ** 2\n",
        "    m_hat = m[i] / (1 - beta1 ** t)\n",
        "    v_hat = v[i] / (1 - beta2 ** t)\n",
        "    updated_params.append(param - learning_rate * m_hat / (np.sqrt(v_hat)))\n"
      ],
      "metadata": {
        "id": "XlGojdeN4KLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Now apply these algorithms to each of the functions that you downloaded to show\n",
        "the impact of changing the following parameters, and discuss:<br>\n",
        "(i) α and β in RMSProp.<br>\n",
        "(ii) α and β in Heavy Ball.<br>\n",
        "(iii) α, β1 and β2 in Adam.<br>"
      ],
      "metadata": {
        "id": "gOLT7jhE4Mor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient of function1 with respect to x and y\n",
        "d_function1_dx = Derivative(function1, x).doit()\n",
        "d_function1_dy = Derivative(function1, y).doit()\n",
        "\n",
        "# Gradient of function2 with respect to x and y\n",
        "d_function2_dx = Derivative(function2, x).doit()\n",
        "d_function2_dy = Derivative(function2, y).doit()\n"
      ],
      "metadata": {
        "id": "ew_X7Qwo5u60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define initial parameters for optimization\n",
        "initial_params = [5, 5]  # Initial values for x and y\n",
        "\n",
        "# Define gradients for each function\n",
        "gradients_function1 = [d_function1_dx.subs({x: initial_params[0], y: initial_params[1]}),\n",
        "                       d_function1_dy.subs({x: initial_params[0], y: initial_params[1]})]\n",
        "gradients_function2 = [d_function2_dx.subs({x: initial_params[0], y: initial_params[1]}),\n",
        "                       d_function2_dy.subs({x: initial_params[0], y: initial_params[1]})]\n",
        "\n",
        "# Apply RMSProp with different parameters\n",
        "# (i) α and β\n",
        "# Let's vary α (learning_rate) and β (decay_rate)\n",
        "learning_rates = [0.01, 0.1, 1]\n",
        "decay_rates = [0.9, 0.99, 0.999]\n",
        "\n",
        "for alpha in learning_rates:\n",
        "  for beta in decay_rates:\n",
        "    # Evaluate gradients numerically\n",
        "    gradients_function1_numerical = gradients_function1.subs({x: initial_params[0], y: initial_params[1]}).evalf()\n",
        "    gradients_function1_numerical = np.array(gradients_function1_numerical)  # Convert to NumPy array\n",
        "\n",
        "    updated_params_rmsprop = rmsprop_update(initial_params, gradients_function1_numerical, alpha, beta, epsilon=1e)\n",
        "\n",
        "# Apply Heavy Ball with different parameters\n",
        "# (ii) α and β\n",
        "# Let's vary α (learning_rate) and β (momentum)\n",
        "learning_rates = [0.01, 0.1, 1]\n",
        "momentum_values = [0.1, 0.5, 0.9]\n",
        "\n",
        "for alpha in learning_rates:\n",
        "    for beta in momentum_values:\n",
        "        updated_params_heavy_ball = heavy_ball_update(initial_params, gradients_function1, alpha, beta)\n",
        "        print(f\"Heavy Ball with alpha={alpha} and beta={beta}: {updated_params_heavy_ball}\")\n",
        "\n",
        "# Apply Adam with different parameters\n",
        "# (iii) α, β1, and β2\n",
        "# Let's vary α (learning_rate), β1, and β2\n",
        "learning_rates = [0.01, 0.1, 1]\n",
        "beta1_values = [0.9, 0.99, 0.999]\n",
        "beta2_values = [0.999, 0.9999, 0.99999]\n",
        "\n",
        "for alpha in learning_rates:\n",
        "    for beta1 in beta1_values:\n",
        "        for beta2 in beta2_values:\n",
        "            updated_params_adam = adam_update(initial_params, gradients_function1, alpha, beta1, beta2, epsilon=1e-8)\n",
        "            print(f\"Adam with alpha={alpha}, beta1={beta1}, and beta2={beta2}: {updated_params_adam}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "0weocTpBg2o0",
        "outputId": "9bfb6953-94d5-4b74-93d8-df00a1b07464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (<ipython-input-27-4e6aa9200e51>, line 22)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-4e6aa9200e51>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    updated_params_rmsprop = rmsprop_update(initial_params, gradients_function1_numerical, alpha, beta, epsilon=1e)\u001b[0m\n\u001b[0m                                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPcMrtC8g3g1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}